# Probability

Status: Done
Assign: GenuineLukas
subject: Math

### Need to know

- pmf vs pdf
    
    PMF → discrete
    
    - probability Mass Function(PMF)
        - $P_X(x)$
        - can take only a finite or at most countable number of values.
    
    PDF → continuous
    
    - probability Density Function (PDF)
        - $f_X(x)$
        - take values on an interval
        - it is always 0 when you get a value for an exact x value because the area of a line is always 0.
        - tells you the rate you accumulate probability around each point.
        - only defined for continuous variables
        - $P(a<X<b) = area\ under\ f_X(x)$
        - $f_X(x)$ needs to satisfy
            - it is defined for all numbers
            - $f_X(x) \ge0$
            - area under $f_X(x) = 1$
- cdf
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%201.png)
    
    Cumulative Distribution Function: Formal Definition 
    
    The CDF shows how much probability the variable has accumulated until a certain value. That means that $F_X(x) = P(X \le x)$
    
    - $F_X(x)$
    - left endpoint is 0
    - right endpoint is 1
    - (endpoints can be infinity)
    - always positive and increasing
- sampling from a distribution
    
    imagine that you have a data set, for example, heights of people, but you need it to be larger, but you can’t go and collect more data, it’s too expensive. So what can you do? 
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%202.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%203.png)
    
- central limit theorem
    
    표본  평균 집합에서 생성한 확률 분포는 표본을 추출한 실제바탕 표본의 형태와는 무관하게 점점 가우스 분포(정규분포)에 접근한다. 
    
    - simple random sample
    - sample size > 30 or population is approximately normal
- law of large numbers
    
    분포에서 추출한 표본의 크기(값들의 개수)가 클 수록 표 평균이 점점 모평균에 가까워진다.
    
- one-hot encoding
    
    빨간색, 녹색, 파란색 같은 색상 자료는 명목형 자료이다. 색상 자료를 심층 신경망에 활용하려면 먼저 그런 데이터를 신경망이 활용할수 있는 형태로 변경해야 한다. 명목형 자료에는 순서가 없으므로 빨간색=0, 녹색=1, 파란색=2 이런식으로  변경하는 것은 신경망이 파란색이 녹색의 2배 이런식으로 수치를 오인할 수 있으므로 바람직하지않다. 심층 신경망에서 명목형 자료를 사용하려면 구간이 의미를 가지는 형태의 데이터로 변환할필요가 있다. 여기사 자주 쓰이는게 one-hot encoding이다. 
    
    원핫 부호화에서는 하나의명목 변수를 하나의벡터로 변환한다. 이 벡터의 성분(요소)들은 명목 변수가 가질 수 있는 값들과 1 대1로 대응된다.
    
    예) 빨간색 → 1 0 0 녹색 → 0 1 0 파란색 → 0 0 1
    
    이러한 백터들은 구간자료에 해당하므로 신경망이 활용할 수 있다. 케라스같은 일부 기계학습 도구 모음은 분류명들에 원핫 부호화를 적용해서 신경망 모형에 전달한다. 
    
- moments of a distribution
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%204.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%205.png)
    
    the cube of the variable detects if you have numbers that are skewed to the left or skewed to the right.
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%206.png)
    
    $$
    Skewness = \mathop{\mathbb{E}}[(\frac{X-\mu}{\sigma})^3]
    $$
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%207.png)
    
- kernel density estimation
    
    [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) is a way to estimate the probability density function. i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights.
    

### Bayes Theorem

In Bayes Theorem problem, we always want to know

$"Posterior"\ \rightarrow P(H|E)$

$$
P(H|E ) = \frac{P(E|H)P(H)}{P(E)}
$$

$$
P(H|E) = \frac{P(H)\cdot P(E|H)}{P(H)P(E|H)\ +\ P(\neg H)P(E|\neg H)}  \ \ \ \ \ \ \ \ \ (1) \newline \newline where \ total \ Probability \ \rightarrow \ P(E) \newline equals\ to\ the\ denominator\ of\ (1)
$$

his is the $evidence$ .

Steve is very shy and withdrawn, invariably helpful but with very little interest in people or in the world or reality. A meek and tidy soul, he has a need for order and structure, and a passion for detail. 

which is more likely? Steve is Librarian or Farmer?

Now, you have a hypothesis: Steve is a Librarian.

You want…

$P(H|E)\ \ \ = \ \ \ P(Hypothesis \ given\ the\ evidence )$

given there are 10 Librarians and 200 farmers,

$"Prior"\ ->\ P(H)\ = \ 1/21$

suppose

$$
Likelihoods \ are \newline \  \newline P(E|H) = 0.4 \newline P(E|\neg H)\ = \ 0.1
$$

Then the equation(1)  comes into play.

remember this.

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%208.png)

some examples 

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%209.png)

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2010.png)

- example problems
    1. Image recognition
        
        getting
        
         $P(cat|image)\ =\ P(cat|pixel_1, pixel_2, \ \ldots , \ pixel_n)$
        
        ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2011.png)
        
    2. Sentiment analysis
        
        
        the first cold shower
        
        even the monkeys seems to want
        
        a little coat of straw
        
        - is this a happy sentence?
        - Calculate P(happy I words in the sentence)
    3. Face generation
        - Generate a group of pixels such that the resulting image looks like a human face.
        - Goal: generate images such that $P(face|pixels)$ is high.

[Spam Detector](Probability%207ae6c16c17904fdc800095c90f656a48/Spam%20Detector%20caa030674ded4d0a8691a1b2668a1143.md)

### Binomial Distribution

“Binomial distribution” with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes-no question, and each with its own Boolean-valued outcome: success(with probability p) or failure(with probability q = 1-p). This is also called the [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).

General PMF for X: number of heads in n coin tosses?

Your coin has P(H) = p

Event: X = x: x heads in n tosses

$P_X(x) = {n \choose x}p^x(1-p)^{n-x}, \ x=0, 1, 2,..., n$

$$
X \ ~\ Binomial(n, p) \newline n \ and \ p \ are\ called\ the\ [arameters\ of\ the\ binomial\ distribution 
$$

- Quiz: What are the parameters for the following binomial distribution
    - I roll 10 dice
    - I want to record the number of times I obtain the number 1

answer → n = 10, p=1/6

- Binomial Coefficient
    
    Unordered sets of length k
    
    $$
    \binom{n}{k} = \frac{n(n-1)(n-2)\dots(n-(k-1))}{k!} = \frac{n!}{(n-k)!k!}
    $$
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2012.png)
    

### Uniform Distribution

motivation: You are calling a tech support lie. They can answer any time between zero and 15 minutes and if they don’t answer in this time, the line is disconnected.

A continuous random variable can be modeled with a uniform distribution of all possible values lie in an interval and have the same frequency of occurrence

Parameters:

- a:beginning of the interval
- b:end of the interval

Uniform Distribution:PDF

$$
P_X(x) = \begin{cases} \frac{1}{b-a} \text \ \ a<x<b\newline 0 \ \ \ \ \ \ x \not\in(a, b) \end{cases}
$$

Uniform Distribution:CDF

$$
F_X(x) = \begin{cases}0 \ \ \ \ \  \ x<a \newline \frac{x-a}{b-a} \ \ a \le x < b \newline 1 \ \ \ \ \ \ b \le x \end{cases}
$$

### Normal Distribution

- Bell Shaped Data
- Range: all the real numbers
- $\mu: Mean$
- $\sigma: standard \ deviation$

$$
\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2} : \ \ X \sim \mathcal{N}(\mu,\,\sigma^{2})
$$

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2013.png)

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2014.png)

the right image is CDF.

- applications: height, weight, IAQ, Noise in a communication channel
- In general, characteristics that are the sum of many independent processes
- Many models in ML are designed under the assumption that the variables follow a normal distribution

### Multivariate Gaussian Distribution

Two variable

$H$: Height of an adult in inches $\sim\mathcal{N(\mu_H, \sigma_H})$

$W$: Weight of an adult in pounds $\sim\mathcal{N(\mu_W, \sigma_W})$

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2015.png)

### Chi-Squared Distribution

Motivation: In communication channel

- Message sent: 10010
- Message received: 10010 + $Z$

Z is noise

interference from other devices./obstructions like walls, tree, etc./Atmosphere conditions: rain, humidity, etc./Electrical interference, i.e. from power lines Others

The communication channel has noise with a standard normal distribution

what is the power of the noise in the channel? $W=Z^2$

what is the distribution of W?

### Joint Distribution

if we want to look at the two variables, for example, the population and the age of the population, that’s two distributions, and we may want to look at them together and see how these two features combined. For that, we need the joint distributions.

$P_{XY}(x, y) = P(X=x, Y=y) \cdots (1)$

for independent discrete random variables:

$P_{XY}=P(X=x, Y=y)=P(x)P(y)\cdots(2)$

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2016.png)

Let X the number rolled on the 1st dice, let Y be the number rolled on the 2nd dice → follows $(2)$

Let X be the number rolled on the 1st dice, let Y be the sum of the 1st and 2nd dices number→

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2017.png)

now.. what about when $X$ and $Y$ are continuous random variables?

let $X$ be the waiting time before a call is picked up [0-10 minutes], let $Y$ be the customer satisfaction rating [0-10]

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2018.png)

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2019.png)

### Marginal and Conditional Distribution

**Marginal Distribution:** Distribution of one variable while ignoring the other.

- joint distribution을 전제로 한다
- joint distribution $P_{X,Y}(X, Y)$를 통해 하나의 확률 변수에대한 확률 함수를구할 수 있다.

$$
P_X(x_i)=\sum_jP_{XY}(x_i, y_j)
\newline
P_X(y_i)=\sum_jP_{XY}(y_i, x_j)
$$

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2020.png)

**Conditional Distribution:** 

$P_{Y|X=x}(y) = \frac{P_{XY}(x, y)}{P_X(x)}$

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2021.png)

### Expected Value

In probability theory, the expected value(also called expectation, expectancy, expectation operator, mathematical expectation, mean, expectation value, or first moment) is a generalization of the **weighted average**.

$\mathop{\mathbb{E}}[X] = x_1p(x_1)+x_2p(x_2)+x_3(x_3)+x_4p(x_4)$ 

$E[f(X)]=f(x_1)p(x_1)+f(x_2)p(x_2)+f(x_3)p(x_3)+f(x_4)p(x_4)$

say

$\mathop{\mathbb{E}}[X_{coin}] = \$0.5 \ \ \ \ \ \mathop{\mathbb{E}}[X_{dice}]=\$3.5$

then the expected value for winning both is 

$$
\mathop{\mathbb{E}}[X]=\mathop{\mathbb{E}}[X_{coin}]+\mathop{\mathbb{E}}[X_{dice}]=\$4
$$

for three people, the expected value for the assignment of a correct name is

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2022.png)

$$
\mathop{\mathbb{E}}[matches ] \ \ = \ \ \mathop{\mathbb{E}}[A]+\mathop{\mathbb{E}}[B]+\mathop{\mathbb{E}}[C]\ \ \ = \frac{1}{3}*3
$$

### Variance

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2023.png)

$Var(X) = \mathop{\mathbb{E}}[(X-\mu)^2]$

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2024.png)

$Var(X) = \mathop{\mathbb{E}}[X^2]-\mathop{\mathbb{E}}[X]^2$

Problems…

say X is measured in meters. Then E[X] is measured in meters. Then the Var(X) is measured in meters^2. This is not so useful.

Then $\sqrt{Var(X)}$ is measured in meters.

Let’s call $std(X)=\sqrt{Var(X)}$, the standard deviation of X.

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2025.png)

[Sum of Gaussians: an Example](Probability%207ae6c16c17904fdc800095c90f656a48/Sum%20of%20Gaussians%20an%20Example%2016b116d1826645d2ac087b4f6e4f8768.md)

**Everything is nicer when the mean is 0**

$$
\mathop{\mathbb{E}}[X+Y]=\mathop{\mathbb{E}}[X]+\mathop{\mathbb{E}}[Y] \newline
\mathop{\mathbb{E}}[X-\mu]=\mathop{\mathbb{E}}[X]-\mathop{\mathbb{E}}[\mu] 
\newline
\ = \mathop{\mathbb{E}}[X] -\mu
\newline\  = 0
$$

**Everything is nicer when the standard deviation is 1**

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2026.png)

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2027.png)

$$
Skewness = \mathop{\mathbb{E}}[(\frac{X-\mu}{\sigma})^3]
$$

[Kurtosis: an Example](Probability%207ae6c16c17904fdc800095c90f656a48/Kurtosis%20an%20Example%20a202412a9bde459cb2d80555ff4f9d61.md)

### Covariance

- Covariance of a Dataset
    
    $Cov(X, Y)=\frac{\sum(x_i - \mu_x)(y_i-\mu_y)}{n}$, for equal probabilities
    
    $Cov(X, Y)=\sum P_{XY}(x_i, y_i)(x_i - \mu_i)(y_i - \mu_i) = \mathop{\mathbb{E}}[XY] -\mathop{\mathbb{E}}[X]\mathop{\mathbb{E}}[Y]$, for unequal probabilities
    
    - n represents the total number of data points across both variables
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2028.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2029.png)
    
- Covariance of a Probability Distribution
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2030.png)
    
    $X:$  how much money in dollars player X wins
    
    $Y:$ how much money in dollars player Y wins
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2031.png)
    
    and variances are 1 for all of them as well. $Var(X_1)\ Var(X_2)\ Var(X_3)\ \ldots Var(Y_3)$
    
    but the covariances are different. 
    
    - covariance for game 1: 1
    - covariance for game 2:-1
    - covariance for game 3: 0
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2032.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2033.png)
    
- Covariance Matrix
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2034.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2035.png)
    

Suppose that the joint probability distribution of two random variables X and Y is given by the table to the right.

What is the covariance between X and Y?

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2036.png)

- answer
    
    the mean of $X$ is 
    
    $$
    \mu_X =(0\times0.2+1\times0.1)+(0\times0.1+1\times0.6)=0.7
    \newline
    $$
    
    And the mean of $Y$ is
    
    $$
    \mu_Y=(0\times0.2+0\times0.1)+(1\times0.1+1\times0.6)=0.7
    $$
    
    Therefore, the covariance between $X$and $Y$ is:
    
    $$
    cov(X, Y)=(0-0.7)(0-0.7)\times0.2\newline
    +(1-0.7)(0-0.7)\times 0.1\newline
    +(0-0.7)(1-0.7)\times0.1\newline
    +(1-0.7)(1-0.7)\times0.6\newline
    =0.11
    $$
    

### Correlation Coefficient

say Age vs Naps per day comparison had the covariance of -7.45.

say Age vs Height comparison had the covariance of 17.

can we say the correlation is much stronger in the latter just because the number is larger? The answer is No.

$$
Correlation\  Coefficient=\frac{Cov(X,Y)}{\sigma_x\sigma_y}=\frac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
$$

- always between -1 and 1

![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2037.png)

### Plots

- Box-Plots
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2038.png)
    
    - the whiskers should never go beyond the max and the minimum values of the data set. So if that happens, like in the example above, simply cut the whisker at those values.
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2039.png)
    
    - if a box plot has a long whisker on the upper end, it is positively skewed.
- Violin Plots
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2040.png)
    
- QQ Plots
    
    Is this data normally distributed? → A Q-Q plot will hep answer that question.
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2041.png)
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2042.png)
    
    the math doesn’t seem so good.
    
    now let’s compare it with the uniform distribution.
    
    ![Untitled](Probability%207ae6c16c17904fdc800095c90f656a48/Untitled%2043.png)
    
    If the poins fall close to the diagonal line, the data is normally distributed.